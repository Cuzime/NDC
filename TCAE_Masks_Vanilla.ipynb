{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tunable Compression Autoencoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.image as mpimg\n",
    "import random\n",
    "import math\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow import keras \n",
    "from keras import layers\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from keras.layers.convolutional.conv2d_transpose import Conv2DTranspose\n",
    "from keras.layers.reshaping.up_sampling2d import UpSampling2D\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbMetricsLogger, WandbModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33magoh\u001b[0m (\u001b[33mcuzime\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.11 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/andrewgoh/Desktop/NDC/wandb/run-20230309_214150-01gyrtm7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cuzime/prelim%20/runs/01gyrtm7' target=\"_blank\">stoic-violet-24</a></strong> to <a href='https://wandb.ai/cuzime/prelim%20' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cuzime/prelim%20' target=\"_blank\">https://wandb.ai/cuzime/prelim%20</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cuzime/prelim%20/runs/01gyrtm7' target=\"_blank\">https://wandb.ai/cuzime/prelim%20/runs/01gyrtm7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# WANDB CONFIG\n",
    "\n",
    "# start run, tracking hyperparameters\n",
    "wandb.init(\n",
    "    #set project\n",
    "    project = \"prelim\",\n",
    "\n",
    "    config = {\n",
    "        \"epochs\": 3,\n",
    "        \"batch_size\": 64,\n",
    "        \"latent_dim\": 16,\n",
    "        \"num_encoding_layers\": 3,\n",
    "        \"num_decoding_layers\": 3,\n",
    "        \"enc_num_filters_1\": 128,\n",
    "        \"enc_num_filters_2\": 64,\n",
    "        \"enc_num_filters_3\": 32,\n",
    "        \"dec_num_filters_1\": 32,\n",
    "        \"dec_num_filters_2\": 64,\n",
    "        \"dec_num_filters_3\": 128,\n",
    "        \"metrics\": \"accuracy\"\n",
    "    }\n",
    "\n",
    ")\n",
    "\n",
    "config = wandb.config\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 28, 28, 1)\n",
      "(60000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# Prepare the dataset. We use both the training & test MNIST digits.\n",
    "\n",
    "(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n",
    "\n",
    "all_digits = np.concatenate([x_train, x_test])\n",
    "all_digits = all_digits.astype(\"float32\") / 255.0\n",
    "all_digits = np.reshape(all_digits, (-1, 28, 28, 1))\n",
    "\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_train = np.reshape(x_train, (-1, 28, 28, 1))\n",
    "\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "x_test = np.reshape(x_test, (-1, 28, 28, 1))\n",
    "\n",
    "print(np.shape(all_digits))\n",
    "print(np.shape(x_train))\n",
    "print(np.shape(x_test))\n",
    "\n",
    "batch_size = config.batch_size\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(all_digits)\n",
    "dataset = dataset.shuffle(buffer_size = 1024).batch(batch_size)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "train_dataset = train_dataset.shuffle(buffer_size = 1024).batch(batch_size)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(x_test)\n",
    "test_dataset = test_dataset.shuffle(buffer_size = 1024).batch(batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range = 13,\n",
    "    zoom_range = 0.1, \n",
    "    shear_range = 10, \n",
    "    width_shift_range = 0.03,\n",
    "    height_shift_range = 0.03,\n",
    ")\n",
    "\n",
    "datagen.fit(x_train)\n",
    "\n",
    "train_generator = datagen.flow(x_train, batch_size = config.batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb0afb92c70>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = random.randint(0, 100)\n",
    "i = random.randint(0, 31)\n",
    "random_image = train_generator.__getitem__(idx)[i].reshape(28,28)\n",
    "\n",
    "\n",
    "plt.imshow(random_image, cmap = 'gray')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TCAE (MASKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 128)       1280      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 28, 28, 128)      512       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 28, 28, 128)       0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 128)      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 14, 14, 64)        73792     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 14, 14, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 7, 7, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 7, 7, 64)          36928     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 7, 7, 64)          0         \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 4, 4, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 4, 4, 32)          18464     \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 4, 4, 32)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                8208      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 139,184\n",
      "Trainable params: 138,928\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1 (Dense)             (None, 1568)              26656     \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 7, 7, 32)          0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 7, 7, 32)         9248      \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 7, 7, 32)          0         \n",
      "                                                                 \n",
      " up_sampling2d (UpSampling2D  (None, 14, 14, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  (None, 14, 14, 32)       9248      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 14, 14, 32)        0         \n",
      "                                                                 \n",
      " up_sampling2d_1 (UpSampling  (None, 28, 28, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2DT  (None, 28, 28, 64)       18496     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 28, 28, 64)        0         \n",
      "                                                                 \n",
      " conv2d_transpose_3 (Conv2DT  (None, 28, 28, 128)      73856     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 28, 28, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 28, 28, 128)       0         \n",
      "                                                                 \n",
      " conv2d_transpose_4 (Conv2DT  (None, 28, 28, 1)        513       \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 28, 28, 1)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 138,529\n",
      "Trainable params: 138,273\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (28,28,1)\n",
    "\n",
    "latent_dim = config.latent_dim\n",
    "\n",
    "# encoder \n",
    "\n",
    "def make_encoder(num_layers, num_filters_1, num_filters_2, num_filters_3):\n",
    "\n",
    "    init_filters = num_filters_1\n",
    "\n",
    "    encoder = keras.Sequential()\n",
    "    encoder.add(keras.Input(shape = input_shape))\n",
    "    encoder.add(layers.Conv2D(init_filters, (3,3), padding = 'same'))\n",
    "    encoder.add(layers.BatchNormalization())\n",
    "    encoder.add(layers.Activation('relu'))\n",
    "    encoder.add(layers.MaxPooling2D((2,2),padding = 'same'))\n",
    "\n",
    "    for i in range(2):\n",
    "        encoder.add(layers.Conv2D(num_filters_2, (3,3), activation = 'relu', padding = 'same'))\n",
    "        encoder.add(layers.Activation('relu'))\n",
    "        encoder.add(layers.MaxPooling2D((2,2),padding = 'same'))\n",
    "\n",
    "    for i in range(num_layers-2):\n",
    "        encoder.add(layers.Conv2D(num_filters_3, (3,3), activation = 'relu', padding = 'same'))\n",
    "        encoder.add(layers.Activation('relu'))\n",
    "\n",
    "    encoder.add(layers.Flatten())\n",
    "    encoder.add(layers.Dense(latent_dim, activation = 'relu'))  \n",
    "\n",
    "    return encoder\n",
    "\n",
    "# decoder\n",
    "\n",
    "def make_decoder(num_layers, num_filters_1, num_filters_2, num_filters_3):\n",
    "\n",
    "    decoder = keras.Sequential()\n",
    "    decoder.add(keras.Input(shape = (latent_dim,)))\n",
    "    decoder.add(layers.Dense(7*7*32, activation = 'relu'))\n",
    "    decoder.add(layers.Reshape(target_shape = (7,7,32)))\n",
    "\n",
    "    for i in range(2):\n",
    "        decoder.add(Conv2DTranspose(num_filters_1, (3,3), padding = 'same'))\n",
    "        decoder.add(layers.Activation('relu'))\n",
    "        decoder.add(UpSampling2D((2,2)))\n",
    "    \n",
    "    for i in range(num_layers-2):\n",
    "        decoder.add(Conv2DTranspose(num_filters_2, (3,3), padding = 'same'))\n",
    "        decoder.add(layers.Activation('relu'))\n",
    "        \n",
    "    decoder.add(Conv2DTranspose(num_filters_3, (3,3), padding = 'same'))\n",
    "    decoder.add(layers.BatchNormalization())\n",
    "    decoder.add(layers.Activation('relu'))\n",
    "\n",
    "    decoder.add(Conv2DTranspose(1, (2,2), padding = 'same'))\n",
    "    decoder.add(layers.Activation('sigmoid'))\n",
    "\n",
    "    return decoder\n",
    "\n",
    "# define number of layers\n",
    "num_encoding_layers = config.num_encoding_layers\n",
    "num_decoding_layers = config.num_decoding_layers\n",
    "\n",
    "# make encoder\n",
    "encoder = make_encoder(num_layers = num_encoding_layers, \n",
    "                       num_filters_1 = config.enc_num_filters_1, \n",
    "                       num_filters_2 = config.enc_num_filters_2, \n",
    "                       num_filters_3 = config.enc_num_filters_3)\n",
    "encoder.summary()\n",
    "\n",
    "'''# get preflattened shape \n",
    "pre_flatten_shape = K.int_shape(encoder.layers[-3].output)[1:]\n",
    "print(pre_flatten_shape)\n",
    "\n",
    "dense_out = 1\n",
    "for elem in pre_flatten_shape:\n",
    "    dense_out *= elem'''\n",
    "\n",
    "# make decoder \n",
    "decoder = make_decoder(num_layers = num_decoding_layers, \n",
    "                       num_filters_1 = config.dec_num_filters_1, \n",
    "                       num_filters_2 = config.dec_num_filters_2, \n",
    "                       num_filters_3 = config.dec_num_filters_3)\n",
    "\n",
    "decoder.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.avg_loss_tracker = keras.metrics.Mean(name = 'avg_loss')\n",
    "        self.avg_accuracy_tracker = keras.metrics.Mean(name = 'avg_accuracy')\n",
    "\n",
    "    def compile(self, optimizer, loss_fn, accuracy_fn):\n",
    "        super().compile()\n",
    "        self.optimizer = optimizer \n",
    "        self.loss_fn = loss_fn\n",
    "        self.accuracy_fn = accuracy_fn\n",
    "\n",
    "    def train_step(self, data):\n",
    "\n",
    "        avg_loss = 0\n",
    "        avg_accuracy = 0\n",
    "\n",
    "        for i in range(int(np.log2(latent_dim)+1)):\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                # create mask\n",
    "                ones_length = int(2**(np.log2(latent_dim)-i))\n",
    "                zeros_length = int(latent_dim - ones_length)\n",
    "                ones = tf.ones((ones_length)) \n",
    "                zeros = tf.zeros((zeros_length))\n",
    "                mask = tf.concat([ones, zeros], axis = 0) \n",
    "                mask = tf.reshape(mask, (1,latent_dim))\n",
    "     \n",
    "                codes = self.encoder(data)\n",
    "                masked_codes = tf.multiply(codes, mask)\n",
    "                #print(codes.shape)\n",
    "                #print(mask.shape)\n",
    "                #print(masked_codes.shape)\n",
    "                #masked_codes = tf.reshape(masked_codes, (tf.shape(codes)[0], latent_dim))\n",
    "                \n",
    "                reconstruction = self.decoder(masked_codes)\n",
    "\n",
    "                loss = self.loss_fn(data, reconstruction)\n",
    "                accuracy = self.accuracy_fn(data, reconstruction)\n",
    "            \n",
    "            avg_loss += loss\n",
    "            avg_accuracy += accuracy\n",
    "            # get gradients\n",
    "            grads = tape.gradient(loss, self.trainable_weights)\n",
    "            # apply gradients\n",
    "            self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "\n",
    "        avg_loss = avg_loss / int(np.log2(latent_dim)+1)\n",
    "        avg_accuracy = avg_accuracy / int(np.log2(latent_dim+1))\n",
    "            \n",
    "        self.avg_loss_tracker.update_state(avg_loss)\n",
    "        self.avg_accuracy_tracker.update_state(avg_accuracy)\n",
    "\n",
    "        return {\n",
    "            'avg_loss': self.avg_loss_tracker.result(),\n",
    "            'avg_accuracy': self.avg_accuracy_tracker.result()\n",
    "        }\n",
    "\n",
    "    def custom_predict(self, data, n):\n",
    "\n",
    "        # create mask\n",
    "        ones_length = int(2**(n))\n",
    "        zeros_length = int(latent_dim - ones_length)\n",
    "        ones = tf.ones((ones_length)) \n",
    "        zeros = tf.zeros((zeros_length))\n",
    "        mask = tf.concat([ones, zeros], axis = 0) \n",
    "        mask = tf.reshape(mask, (1,latent_dim))\n",
    "\n",
    "        codes = self.encoder(data)\n",
    "        masked_codes = tf.multiply(codes, mask)\n",
    "        #masked_codes = tf.reshape(masked_codes, (1, latent_dim))\n",
    "        \n",
    "        reconstruction = self.decoder(masked_codes)\n",
    "\n",
    "        return reconstruction\n",
    "\n",
    "    def test_step(self, data):\n",
    "        \n",
    "        avg_loss = 0\n",
    "        avg_accuracy = 0\n",
    "\n",
    "        for i in range(int(np.log2(latent_dim)+1)):\n",
    "            # create mask\n",
    "            ones_length = int(2**(np.log2(latent_dim)-i))\n",
    "            zeros_length = int(latent_dim - ones_length)\n",
    "            ones = tf.ones((ones_length)) \n",
    "            zeros = tf.zeros((zeros_length))\n",
    "            mask = tf.concat([ones, zeros], axis = 0) \n",
    "            mask = tf.reshape(mask, (1,latent_dim))\n",
    "\n",
    "            # mask codes\n",
    "            codes = self.encoder(data)\n",
    "            masked_codes = tf.multiply(codes, mask)        \n",
    "            reconstruction = self.decoder(masked_codes)\n",
    "\n",
    "            loss = self.loss_fn(data, reconstruction)\n",
    "            accuracy = self.accuracy_fn(data, reconstruction)\n",
    "            \n",
    "            avg_loss += loss\n",
    "            avg_accuracy += accuracy\n",
    "        \n",
    "        avg_loss = avg_loss / int(np.log2(latent_dim)+1)\n",
    "        avg_accuracy = avg_accuracy / int(np.log2(latent_dim+1))\n",
    "            \n",
    "        self.avg_loss_tracker.update_state(avg_loss)\n",
    "        self.avg_accuracy_tracker.update_state(avg_accuracy)\n",
    "\n",
    "\n",
    "        return {\n",
    "            'avg_loss': self.avg_loss_tracker.result(),\n",
    "            'avg_accuracy': self.avg_accuracy_tracker.result()\n",
    "        }\n",
    "    \n",
    "    def call(self, data):\n",
    "        codes = self.encoder(data)\n",
    "        reconstruction = self.decoder(codes)\n",
    "\n",
    "        return reconstruction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# test masking \n",
    "\n",
    "i = 2\n",
    "\n",
    "ones_length = int(2**(i))\n",
    "zeros_length = int(latent_dim - ones_length)\n",
    "ones = tf.ones((ones_length)) \n",
    "zeros = tf.zeros((zeros_length))\n",
    "mask = tf.concat([ones, zeros], axis = 0)\n",
    "mask = tf.reshape(mask, (1, latent_dim,))\n",
    "\n",
    "a = mask.numpy()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize tcae\n",
    "\n",
    "tcae = TCAE(encoder = encoder, decoder = decoder)\n",
    "tcae.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001), \n",
    "             loss_fn = tf.keras.losses.binary_crossentropy, \n",
    "             accuracy_fn = tf.keras.metrics.BinaryAccuracy(name=\"binary_accuracy\", dtype=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m When using `save_best_only`, ensure that the `filepath` argument contains formatting placeholders like `{epoch:02d}` or `{batch:02d}`. This ensures correct interpretation of the logged artifacts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "938/938 [==============================] - ETA: 0s - avg_loss: 0.2144 - avg_accuracy: 0.9201"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./models)... Done. 0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 1982s 2s/step - avg_loss: 0.2144 - avg_accuracy: 0.9201 - val_avg_loss: 0.1792 - val_avg_accuracy: 1.0096\n",
      "Epoch 2/3\n",
      "938/938 [==============================] - ETA: 0s - avg_loss: 0.1945 - avg_accuracy: 0.9218"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./models)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 1704s 2s/step - avg_loss: 0.1945 - avg_accuracy: 0.9218 - val_avg_loss: 0.1704 - val_avg_accuracy: 1.0120\n",
      "Epoch 3/3\n",
      "938/938 [==============================] - ETA: 0s - avg_loss: 0.1897 - avg_accuracy: 0.9220"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./models)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 1740s 2s/step - avg_loss: 0.1897 - avg_accuracy: 0.9220 - val_avg_loss: 0.1651 - val_avg_accuracy: 1.0139\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch/avg_accuracy</td><td>▁▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆█▇▇▆▆▆▆▆▆▆▆▆▆</td></tr><tr><td>batch/avg_loss</td><td>█▆▅▄▄▄▃▃▃▃▃▃▃▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>batch/batch_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>batch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/avg_accuracy</td><td>▁██</td></tr><tr><td>epoch/avg_loss</td><td>█▂▁</td></tr><tr><td>epoch/epoch</td><td>▁▅█</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁</td></tr><tr><td>epoch/val_avg_accuracy</td><td>▁▅█</td></tr><tr><td>epoch/val_avg_loss</td><td>█▄▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch/avg_accuracy</td><td>0.92196</td></tr><tr><td>batch/avg_loss</td><td>0.18972</td></tr><tr><td>batch/batch_step</td><td>2815</td></tr><tr><td>batch/learning_rate</td><td>0.001</td></tr><tr><td>epoch/avg_accuracy</td><td>0.92196</td></tr><tr><td>epoch/avg_loss</td><td>0.18972</td></tr><tr><td>epoch/epoch</td><td>2</td></tr><tr><td>epoch/learning_rate</td><td>0.001</td></tr><tr><td>epoch/val_avg_accuracy</td><td>1.01394</td></tr><tr><td>epoch/val_avg_loss</td><td>0.16508</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">stoic-violet-24</strong> at: <a href='https://wandb.ai/cuzime/prelim%20/runs/01gyrtm7' target=\"_blank\">https://wandb.ai/cuzime/prelim%20/runs/01gyrtm7</a><br/>Synced 6 W&B file(s), 0 media file(s), 15 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230309_214150-01gyrtm7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = config.epochs\n",
    "\n",
    "tcae.fit(train_generator, epochs = epochs, validation_data = test_dataset,\n",
    "                   callbacks = [\n",
    "                    WandbMetricsLogger(log_freq=5),\n",
    "                    WandbModelCheckpoint(\"models\", save_best_only = True, monitor = 'val_avg_loss')\n",
    "                   ])\n",
    "wandb.finish()\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"save weights\"\"\"\n",
    "#tcae.save_weights('/Users/andrewgoh/Desktop/NDC/weights/tcae_weights', overwrite=True, save_format=None, options=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"load weights\"\"\"\n",
    "\"\"\"tcae = TCAE(encoder = encoder, decoder = decoder)\n",
    "tcae.load_weights('/Users/andrewgoh/Desktop/NDC/weights/tcae_weights', by_name=False, skip_mismatch=False, options=None)\n",
    "\n",
    "# set optimizer and compile \n",
    "tcae.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001), loss_fn = tf.keras.losses.BinaryCrossentropy())\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1152x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = random.randint(0, np.shape(all_digits)[0]-1)\n",
    "img = all_digits[index,:,:,0]\n",
    "\n",
    "plt.figure(figsize = (3,3))\n",
    "plt.imshow(img)\n",
    "\n",
    "img = img.reshape(1, 28, 28, 1)\n",
    "\n",
    "plt.figure(figsize = (16,5))\n",
    "for i in range(int(np.log2(latent_dim))+1):\n",
    "    ax = plt.subplot(2, 8, i + 1)\n",
    "    reconstruction = tcae.custom_predict([img], np.log2(latent_dim)-i)\n",
    "    reconstruction = reconstruction.numpy().reshape(28,28)\n",
    "    plt.title('latent dim:{}'.format(int(2**(np.log2(latent_dim)-i))))\n",
    "    plt.imshow(reconstruction)\n",
    "    plt.axis('off')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
